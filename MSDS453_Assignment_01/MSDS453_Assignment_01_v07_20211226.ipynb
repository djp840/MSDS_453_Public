{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/djp840/MSDS_453_Public/blob/main/images/NorthwesternHeader.png?raw=1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSDS453 - Research Assignment 01 - First Vectorized Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Our goal in this exercise is to BEGIN coming to a common agreement, among this class, as to what terms we will use as we selectively refine our corpus-wide vocabulary. This corpus vocabulary is what would represent the content of each different document for clustering and classification purposes, which will be our next step. This means that we need to make decisions - what is in, what is out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from packaging import version\n",
    "\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import ngrams, FreqDist\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, silhouette_score\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec,LdaMulticore, TfidfModel\n",
    "from gensim import corpora\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Gensim</b> is a Python library for topic modelling, document indexing and similarity retrieval with large corpora. Target audience is the natural language processing (NLP) and information retrieval (IR) community <br><br>\n",
    "    <b>https://pypi.org/project/gensim/ </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "pkg_resources.require(\"gensim<=3.8.3\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Genism Version: \", gensim.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Only run this once, they will be downloaded.\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create List of Stop Words from NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mount Google Drive to Colab Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot Encoding Example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define example\n",
    "data = ['ginsburg', 'RBG', 'justice', 'ginsburg']\n",
    "values = array(data)\n",
    "print(values)\n",
    "# integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(values)\n",
    "print(integer_encoded)\n",
    "# binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "print(onehot_encoded)\n",
    "# invert first example\n",
    "#print(onehot_encoded[0, :])\n",
    "inverted = label_encoder.inverse_transform([argmax(onehot_encoded[0, :])])\n",
    "#print(inverted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Data Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process dataset to remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(in_text):\n",
    "    # Remove punctuation\n",
    "    text = re.sub('[^a-zA-Z]', ' ', str(in_text))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process dataset to lower case it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_case(in_text):\n",
    "    # Convert to lowercase\n",
    "    text = in_text.lower()    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process dataset to remove tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tags(in_text):    \n",
    "    # Remove tags\n",
    "    text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",in_text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process dataset to remove special characters and digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_special_chars_and_digits(in_text):\n",
    "    # Remove special characters and digits\n",
    "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",in_text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process dataset to appy Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_stemming(in_text):\n",
    "    stemmer=PorterStemmer()\n",
    "    word_list = nltk.word_tokenize(in_text)\n",
    "    output = ' '.join([stemmer.stem(w) for w in word_list])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process dataset to apply Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lemmatization(in_text):\n",
    "    # Lemmatization\n",
    "    lem = WordNetLemmatizer()\n",
    "    word_list = nltk.word_tokenize(in_text)\n",
    "    output = ' '.join([lem.lemmatize(w) for w in word_list])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(in_text):\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    word_tokens = word_tokenize(in_text)  \n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "    filtered_sentence = [] \n",
    "    for w in word_tokens: \n",
    "        if w not in stop_words: \n",
    "            filtered_sentence.append(w) \n",
    "\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Phase Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_phrase_machine(in_text):\n",
    "    phrases=phrasemachine.get_phrases(in_text)\n",
    "    return phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Rake Keyword Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rake(in_text):\n",
    "    r = Rake()\n",
    "    r.extract_keywords_from_text(in_text)\n",
    "    rake_phrases= r.get_ranked_phrases()\n",
    "    return rake_phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run NLTK Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_nltk_tokenizer(in_text):\n",
    "    tokens=nltk.word_tokenize(in_text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run NLTK Sentence Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_nltk_sent_tokenizer(in_corpus):\n",
    "    sents = nltk.sent_tokenize(in_corpus)\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run word-ngram Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_nltk_tokenizer_word_ngrams(in_text, ngram_size):\n",
    "    n_grams = ngrams(nltk.word_tokenize(in_text), ngram_size)\n",
    "    return [ ' '.join(grams) for grams in n_grams]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Frequency Distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_freq_dist(terms):\n",
    "    all_counts = dict()\n",
    "    all_counts[size] = FreqDist(terms)\n",
    "    return all_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Text into Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus(path_to_file):\n",
    "    #read in class corpus csv into python\n",
    "    data=pd.read_csv(path_to_file)\n",
    "\n",
    "    #create empty list to store text documents titles\n",
    "    titles=[]\n",
    "    docids=[]\n",
    "    #for loop which appends the DSI title to the titles list\n",
    "    for i in range(0,len(data)):\n",
    "        docids.append(data['Doc_ID'].iloc[i])\n",
    "\n",
    "    #for loop which appends the DSI title to the titles list\n",
    "    for i in range(0,len(data)):\n",
    "        titles.append(data['DSI_Title'].iloc[i])\n",
    "\n",
    "    #create empty list to store text documents\n",
    "    text_body=[]\n",
    "\n",
    "    #for loop which appends the text to the text_body list\n",
    "    for i in range(0,len(data)):\n",
    "        temp_text=data['Text'].iloc[i]\n",
    "        text_body.append(temp_text)\n",
    "    return (docids,titles,text_body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to visualize documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_doc(doc): \n",
    "    #split document into individual words\n",
    "    doc = ' '.join(remove_stop_words(doc))\n",
    "    doc = apply_lemmatization(doc)\n",
    "\n",
    "    tokens=doc.split()\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation from each word\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 4]\n",
    "    #lowercase all words\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def process_corpus(data):\n",
    "        \n",
    "    #create empty list to store text documents titles\n",
    "    titles=[]\n",
    "    \n",
    "    #for loop which appends the DSI title to the titles list\n",
    "    for i in range(0,len(data)):\n",
    "        temp_text=data['DSI_Title'].iloc[i]\n",
    "        titles.append(temp_text)\n",
    "    \n",
    "    #create empty list to store text documents\n",
    "    text_body=[]\n",
    "    \n",
    "    #for loop which appends the text to the text_body list\n",
    "    for i in range(0,len(data)):\n",
    "        temp_text=data['Text'].iloc[i]\n",
    "        text_body.append(temp_text)\n",
    "    \n",
    "    #Note: the text_body is the unprocessed list of documents read directly form \n",
    "    #the csv.\n",
    "        \n",
    "    #empty list to store processed documents\n",
    "    processed_text=[]\n",
    "    #for loop to process the text to the processed_text list\n",
    "    for i in text_body:\n",
    "        text=clean_doc(i)\n",
    "        processed_text.append(text)\n",
    "    \n",
    "    #Note: the processed_text is the PROCESSED list of documents read directly form \n",
    "    #the csv.  Note the list of words is separated by commas.\n",
    "    \n",
    "    \n",
    "    #stitch back together individual words to reform body of text\n",
    "    final_processed_text=[]\n",
    "    \n",
    "    for i in processed_text:\n",
    "        temp_DSI=i[0]\n",
    "        for k in range(1,len(i)):\n",
    "            temp_DSI=temp_DSI+' '+i[k]\n",
    "        final_processed_text.append(temp_DSI)\n",
    "    \n",
    "    return titles, final_processed_text, processed_text\n",
    "\n",
    "def run_doc2vec(final_processed_text, processed_text):\n",
    "    #create doc2vec matrix\n",
    "    documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(final_processed_text)]\n",
    "    model = Doc2Vec(documents, vector_size=100, window=3, min_count=2, workers=4)    \n",
    "    \n",
    "    doc2vec_df=pd.DataFrame()\n",
    "    for i in range(0,len(processed_text)):\n",
    "        vector=pd.DataFrame(model.infer_vector(processed_text[i])).transpose()\n",
    "        doc2vec_df=pd.concat([doc2vec_df,vector], axis=0)\n",
    "        \n",
    "    return doc2vec_df\n",
    "\n",
    "def run_word2vec(processed_text):\n",
    "\n",
    "    #word to vec model\n",
    "    model_w2v = Word2Vec(processed_text, size=100, window=3, min_count=2, workers=4)\n",
    "    \n",
    "    labels = []\n",
    "    tokens = []\n",
    "\n",
    "    for word in model_w2v.wv.vocab:\n",
    "        tokens.append(model_w2v[word])\n",
    "        labels.append(word)\n",
    "    \n",
    "    \n",
    "    return labels, tokens, model_w2v\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_doc2vec(doc2vec_df, titles,_perplexity=10):\n",
    "    #visualize documents through applying TSNE to doc2vec matrix\n",
    "    tsne_model = TSNE(perplexity=_perplexity, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "    doc2vec_df = tsne_model.fit_transform(doc2vec_df)\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in doc2vec_df:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "\n",
    "    plt.figure(figsize=(16, 16)) \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(titles[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def plot_word2vec(model, labels, tokens, _perplexity=30):\n",
    "    \"Creates and TSNE model and plots it\"\n",
    "    words = []\n",
    "    embeddings = []\n",
    "    for word in list(model.wv.vocab):\n",
    "        embeddings.append(model.wv[word])\n",
    "        words.append(word)\n",
    "\n",
    "    tsne_model= TSNE(perplexity=_perplexity, n_components=2, init='pca', n_iter=3500, random_state=32)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=(40, 40)) \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source Class Corpus  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "path_to_file='change me /path/to/source/data/Class_Corpus_v2.csv'\n",
    "docids,titles,text=get_corpus(path_to_file)\n",
    "print(titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retreive Word Count Frequencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range = (1,1))\n",
    "matrix=vectorizer.fit_transform(text)     \n",
    "\n",
    "#creating datafram from TFIDF Matrix\n",
    "words = vectorizer.get_feature_names()\n",
    "matrix=pd.DataFrame(matrix.toarray(), columns=vectorizer.get_feature_names(), index=titles)\n",
    "frequencies = matrix.sum().transpose().reset_index()\n",
    "frequencies.columns = ['word', 'counts']\n",
    "frequencies.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculates TFIDF and Saves TFIDF Values for Terms "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output Directory Pathway - Update to Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', use_idf=True, norm=None)\n",
    "transformed_documents = vectorizer.fit_transform(text)\n",
    "transformed_documents_as_array = transformed_documents.toarray()\n",
    "\n",
    "####################################################################################\n",
    "# NOTE THIS SAVE A FILE FOR EACH DOCUMENT TO YOUR HARD DRIVE\n",
    "# It first creates a directory called td_idf_output\n",
    "# Then for each document it will generate a file with the words and tf idf scores\n",
    "###################################################################################\n",
    "output_dir =\"change me /path/to/source/output data/TFIDF_output\"\n",
    "\n",
    "import os\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "for counter, doc in enumerate(transformed_documents_as_array):\n",
    "    # construct a dataframe\n",
    "    tf_idf_tuples = list(zip(vectorizer.get_feature_names(), doc))\n",
    "    one_doc_as_df = pd.DataFrame.from_records(tf_idf_tuples\n",
    "                                              ,columns=['term', 'score']).sort_values(by='score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # output to a csv using the enumerated value for the filename\n",
    "    one_doc_as_df.to_csv(output_dir+\"/\"+str(titles[counter]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(path_to_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Word and Document Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleans text to get processed text\n",
    "titles, final_processed_text, processed_text = process_corpus(data)\n",
    "\n",
    "\n",
    "#word to vec\n",
    "model_w2v = Word2Vec(processed_text, size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "#join all processed DSI words into single list\n",
    "processed_text_w2v=[]\n",
    "for i in processed_text:\n",
    "    for k in i:\n",
    "        processed_text_w2v.append(k)\n",
    "\n",
    "#obtian all the unique words from DSI\n",
    "w2v_words=list(set(processed_text_w2v))\n",
    "\n",
    "#can also use the get_feature_names() from TFIDF to get the list of words\n",
    "#w2v_words=Tfidf.get_feature_names()\n",
    "\n",
    "#empty dictionary to store words with vectors\n",
    "w2v_vectors={}\n",
    "\n",
    "#for loop to obtain weights for each word\n",
    "for i in w2v_words:\n",
    "    temp_vec=model_w2v.wv[i]\n",
    "    w2v_vectors[i]=temp_vec\n",
    "\n",
    "#create a final dataframe to view word vectors\n",
    "w2v_df=pd.DataFrame(w2v_vectors).transpose()\n",
    "\n",
    "w2v_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Doc2Vec Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(final_processed_text)]\n",
    "model = Doc2Vec(documents, vector_size=1000, window=2, min_count=1, workers=4)\n",
    "\n",
    "doc2vec_df=pd.DataFrame()\n",
    "for i in range(0,len(processed_text)):\n",
    "    vector=pd.DataFrame(model.infer_vector(processed_text[i])).transpose()\n",
    "    doc2vec_df=pd.concat([doc2vec_df,vector], axis=0)\n",
    "\n",
    "doc2vec_df=doc2vec_df.reset_index()\n",
    "\n",
    "doc_titles={'title': titles}\n",
    "t=pd.DataFrame(doc_titles)\n",
    "\n",
    "doc2vec_df=pd.concat([t, doc2vec_df], axis=1)\n",
    "\n",
    "doc2vec_df=doc2vec_df.drop('index', axis=1)\n",
    "\n",
    "doc2vec_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculates the Cosine Similarity across your corpus using TFIDF matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_term_matrix = transformed_documents.todense()\n",
    "df = pd.DataFrame(doc_term_matrix, \n",
    "                      columns=vectorizer.get_feature_names(), \n",
    "                      index=docids)\n",
    "   \n",
    "similarity  = cosine_similarity(df, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap of Cosine Similarity of Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity  = cosine_similarity(df, df)\n",
    "a4_dims = (30, 30)\n",
    "fig, ax = pyplot.subplots(figsize=a4_dims)\n",
    "sns.heatmap(ax=ax, data=similarity, xticklabels= titles, yticklabels=titles);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Class Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output Directory Pathway - Update to Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(path_to_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Documents\n",
    "1. Process documents\n",
    "2. Create doc2vec matrix\n",
    "3. Plotting with TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process documents\n",
    "titles, final_processed_text, processed_text = process_corpus(data)\n",
    "print(final_processed_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec_df = run_doc2vec(final_processed_text, processed_text)\n",
    "labels, tokens, word2vec_model = run_word2vec(processed_text)\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot doc2vec TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_doc2vec(doc2vec_df, titles,15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot word2vec TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "plot_word2vec(word2vec_model, labels, tokens,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore K-means clustering of TFIDF matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "#Calculates tf idf\n",
    "#Edit ngram range if you like\n",
    "#############################################\n",
    "def tfidf(corpus, titles, ngram_range = (1,1)):\n",
    "    #this is a function to created the tfidf matrix\n",
    "    Tfidf=TfidfVectorizer(stop_words='english', ngram_range = ngram_range)\n",
    "\n",
    "    #fit the vectorizer using final processed documents.  The vectorizer requires the \n",
    "    #stiched back together document.\n",
    "\n",
    "    TFIDF_matrix=Tfidf.fit_transform(corpus)     \n",
    "\n",
    "    #creating datafram from TFIDF Matrix\n",
    "    words = Tfidf.get_feature_names()\n",
    "    matrix=pd.DataFrame(TFIDF_matrix.toarray(), columns=Tfidf.get_feature_names(), index=titles)\n",
    "    return matrix, words\n",
    "\n",
    "#############################################\n",
    "#Performs the k-means clustering of the tfidf matrix\n",
    "#Edit the number of clusters\n",
    "#############################################\n",
    "def k_means_tfidf(tfidf_matrix,terms,titles,final_processed_text, k=10):\n",
    "    \n",
    "    #this is a function to generate the k-means output using the tfidf matrix.  Inputs \n",
    "    #to the function include: titles of text, processed text, and desired k value. \n",
    "    km = KMeans(n_clusters=k, random_state =89)\n",
    "    km.fit(tfidf_matrix)\n",
    "    clusters = km.labels_.tolist()\n",
    "\n",
    "    Dictionary={'Doc Name':titles, 'Cluster':clusters,  'Text': final_processed_text}\n",
    "    frame=pd.DataFrame(Dictionary, columns=['Cluster', 'Doc Name','Text'])\n",
    "\n",
    "    print(\"Top terms per cluster:\")\n",
    "    #sort cluster centers by proximity to centroid\n",
    "    order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "\n",
    "    terms_dict=[]\n",
    "\n",
    "\n",
    "    #save the terms for each cluster and document to dictionaries.  To be used later\n",
    "    #for plotting output.\n",
    "\n",
    "    #dictionary to store terms and titles\n",
    "    cluster_terms={}\n",
    "    cluster_title={}\n",
    "\n",
    "\n",
    "    for i in range(k):\n",
    "        print(\"Cluster %d:\" % i),\n",
    "        temp_terms=[]\n",
    "        temp_titles=[]\n",
    "        for ind in order_centroids[i, :10]:\n",
    "            print(' %s' % terms[ind])\n",
    "            terms_dict.append(terms[ind])\n",
    "            temp_terms.append(terms[ind])\n",
    "        cluster_terms[i]=temp_terms\n",
    "\n",
    "        print(\"Cluster %d titles:\" % i, end='')\n",
    "        temp=frame[frame['Cluster']==i]\n",
    "        for title in temp['Doc Name']:\n",
    "            print(' %s,' % title, end='')\n",
    "            temp_titles.append(title)\n",
    "        cluster_title[i]=temp_titles\n",
    "        \n",
    "\n",
    "#############################################\n",
    "# Run the code\n",
    "#############################################\n",
    "tfidf_matrix, terms = tfidf(final_processed_text, titles)\n",
    "k_means_tfidf(tfidf_matrix,terms,titles,final_processed_text, k=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore K-means clustering of doc2vec matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def doc2vec_with_titles(corpus, processed_text, titles, vector_size = 100):\n",
    "    #function to created doc2vec matrix\n",
    "    documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(corpus)]\n",
    "\n",
    "    model = Doc2Vec(documents, vector_size=100, window=3, min_count=2, workers=4)\n",
    "\n",
    "    doc2vec_df=pd.DataFrame()\n",
    "    for i in range(0,len(processed_text)):\n",
    "        vector=pd.DataFrame(model.infer_vector(processed_text[i])).transpose()\n",
    "        doc2vec_df=pd.concat([doc2vec_df,vector], axis=0)\n",
    "\n",
    "    doc2vec_df=doc2vec_df.reset_index()\n",
    "\n",
    "    doc_titles={'title': titles}\n",
    "    t=pd.DataFrame(doc_titles)\n",
    "\n",
    "    doc2vec_df=pd.concat([doc2vec_df,t], axis=1)\n",
    "\n",
    "    doc2vec_df=doc2vec_df.drop('index', axis=1)\n",
    "    return doc2vec_df\n",
    "\n",
    "def k_means_doc2vec(doc2vec_df, final_processed_text,titles, k = 5):\n",
    "    \n",
    "    #this is a funciton to create the k_means outputs using the doc2vec matrics.  Required inputs\n",
    "    #include the doc2vec matrix, and desired number of clusters.\n",
    "    doc2vec_k_means=doc2vec_df.drop('title', axis=1)\n",
    "\n",
    "    km = KMeans(n_clusters=k, random_state =89)\n",
    "    km.fit(doc2vec_k_means)\n",
    "\n",
    "    clusters_d2v = km.labels_.tolist()\n",
    "\n",
    "    Dictionary={'Doc Name':titles, 'Cluster':clusters_d2v,  'Text': final_processed_text}\n",
    "    frame=pd.DataFrame(Dictionary, columns=['Cluster', 'Doc Name','Text'])\n",
    "\n",
    "    #dictionary to store clusters and respective titles\n",
    "    cluster_title={}\n",
    "\n",
    "    #note doc2vec clusters will not have individual words due to the vector representation\n",
    "    #is based on the entire document not indvidual words. As a result, there won't be individual\n",
    "    #word outputs from each cluster.   \n",
    "    for i in range(k):\n",
    "        print(\"Cluster %d:\" % i)\n",
    "        temp=frame[frame['Cluster']==i]\n",
    "        temp_title_list=[]\n",
    "        for title in temp['Doc Name']:\n",
    "            print(' %s ' % title, end='\\n')\n",
    "            temp_title_list.append(title)\n",
    "        cluster_title[i]=temp_title_list\n",
    "\n",
    "doc2vec_model = doc2vec_with_titles(final_processed_text, processed_text,titles)\n",
    "doc2vec_model.index = titles\n",
    "k_means_doc2vec(doc2vec_model, final_processed_text,titles, k =15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of Silhouette Score \n",
    "Measurement most optimal number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_list = [2, 5, 7, 10]\n",
    "#silhoutte score list\n",
    "sil_scores = []\n",
    "\n",
    "for k in k_list:\n",
    "    km = KMeans(n_clusters=k, random_state =89)\n",
    "    km.fit(tfidf_matrix)\n",
    "    labels = km.labels_.tolist()\n",
    "    score = silhouette_score(tfidf_matrix, labels)\n",
    "    sil_scores.append(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Silhouette Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "plt.scatter(k_list, sil_scores)\n",
    "plt.plot(k_list, sil_scores)\n",
    "plt.xlabel(\"clusters\")\n",
    "plt.ylabel(\"sillhouette score\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
